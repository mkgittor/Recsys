# -*- coding: utf-8 -*-
"""Recsys_3_itembasedCF_SVDNMF_withEval.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13aGkZbEDvK2h-ttEuoXrJrYZyM-2R1GY

### Importing data and basic Python libraries
"""

!wget "http://files.grouplens.org/datasets/movielens/ml-1m.zip"
!unzip /content/ml-1m.zip
!head ml-1m/movies.dat -n 10
!head ml-1m/ratings.dat -n 10
!head ml-1m/users.dat -n 10

import re
import csv
import codecs
import numpy as np
import pandas as pd

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

from torch.utils.data import DataLoader, TensorDataset
import torch
import torch.nn as nn
import torch.nn.functional as F
import spacy

from gensim.models import KeyedVectors


import torch
import torch.nn as nn
from torch.autograd import Variable

#import spacy
#import torchtext
#from torchtext import data
#from torchtext.data import Field, BucketIterator, TabularDataset

from sklearn.model_selection import train_test_split

# from Batch import MyIterator, batch_size_fn
# from Tokenize import tokenize

import os

#import numpy as np
#import pandas as pd

import math
import copy

#import torch.nn.functional as F
import time

"""### Exploring dataset and reading into a dataframe"""

# Movies dataframe

df_movies = pd.read_csv(r'ml-1m/movies.dat',sep='::',names=['movieId','title','genres'], engine='python')

#df_movies.head(10)
#df_movies.shape[0]

# 1M .dat file : 3,883 movies
# Make a census of the genre keywords
genre_labels = set()
for s in df_movies['genres'].str.split('|').values:
    genre_labels = genre_labels.union(set(s))

# Function that counts the number of times each of the genre keywords appear
def count_word(dataset, ref_col, census):
    keyword_count = dict()
    for s in census: 
        keyword_count[s] = 0
    for census_keywords in dataset[ref_col].str.split('|'):        
        if type(census_keywords) == float and pd.isnull(census_keywords): 
            continue        
        for s in [s for s in census_keywords if s in census]: 
            if pd.notnull(s): 
                keyword_count[s] += 1
    #______________________________________________________________________
    # convert the dictionary in a list to sort the keywords by frequency
    keyword_occurences = []
    for k,v in keyword_count.items():
        keyword_occurences.append([k,v])
    keyword_occurences.sort(key = lambda x:x[1], reverse = True)
    return keyword_occurences, keyword_count

# Calling this function gives access to a list of genre keywords which are sorted by decreasing frequency
keyword_occurences, dum = count_word(df_movies, 'genres', genre_labels)
keyword_occurences[:5]

# Users dataframe

df_users = pd.read_csv(r'ml-1m/users.dat',sep='::',names=['userId','Gender','Age','Occ','Zipcode'], engine='python')

#Age_dictionary = {1 : 'Under 18', 18 : '18-24', 25 : '25-34', 35 : '35-44', 45 : '45-49', 50 : '50-55', 56 : '56+'}
#Occ_Dictionary = {0 : 'other or not specified', 1 : 'academic/educator', 2 : 'artist', 3 : 'clerical/admin', 4 : 'college/grad student',
#                 5 : 'customer service', 6 : 'doctor/health care', 7 : 'executive/managerial', 8 : 'farmer', 9 : 'homemaker', 10 : 'K-12 student',
#                 11 : 'lawyer', 12 : 'programmer', 13 : 'retired', 14 : 'sales/marketing', 15 : 'scientist', 16 : 'self-employed', 17 : 'technician/engineer',
#                 18 : 'tradesman/craftsman', 19 : 'unemployed', 20 : 'writer'}
#df_users['Age_Desc'] = df_users['Age'].map(Age_dictionary)
#df_users['Occ_Desc'] = df_users['Occ'].map(Occ_Dictionary)

#df_users=df_users.drop(['Age','Occ'],axis = 1)

#df_users.head(10)
#df_users.shape[0]

#print (df_users.info())

# ml-1M : 6,040 users

# Ratings Data frame

df_ratings = pd.read_csv(r'ml-1m/ratings.dat',sep='::',names=['userId','movieId','rating','timestamp'], engine='python')

#df_ratings.head(10)
#df_ratings.shape[0]

# ml-1M : 1,000,209 ratings

# Get summary statistics of rating
#df_ratings['rating'].describe()

# Import seaborn library
#import seaborn as sns
#sns.set_style('whitegrid')
#sns.set(font_scale=1.5)
#%matplotlib inline

# Display distribution of rating
#sns.distplot(df_ratings['rating'].fillna(df_ratings['rating'].median()))

# Join all 3 files into one dataframe
#dataset = pd.merge(pd.merge(df_movies, df_ratings),df_users)
# Display 20 movies with highest ratings
#dataset[['title','genres','rating']].sort_values('rating', ascending=False).head(20)

"""### Wordcloud Exploration with new libraraies"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import wordcloud
from wordcloud import WordCloud, STOPWORDS

# Create a wordcloud of the Movie titles 

df_movies['title'] = df_movies['title'].fillna("").astype('str')
title_corpus = ' '.join(df_movies['title'])
title_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', height=2000, width=4000).generate(title_corpus)

# Plot the wordcloud
plt.figure(figsize=(16,8))
plt.imshow(title_wordcloud)
plt.axis('off')
plt.show()


# Define the dictionary used to produce the genre wordcloud
genres = dict()
trunc_occurences = keyword_occurences[0:18]
for s in trunc_occurences:
    genres[s[0]] = s[1]

# Create the wordcloud
genre_wordcloud = WordCloud(width=1000,height=400, background_color='white')
genre_wordcloud.generate_from_frequencies(genres)

# Plot the wordcloud
f, ax = plt.subplots(figsize=(16, 8))
plt.imshow(genre_wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""### Explore Data set and join records"""

# Number of unique users and movies

#df_movies.head(10)
#df_ratings.head(10)

n_users = df_ratings.userId.unique().shape[0]
n_movies = df_ratings.movieId.unique().shape[0]
print('Number of users = ' + str(n_users) + ' | Number of movies = ' + str(n_movies))

# Create new dataframe Ratings with userId in rows and movie name in columns

Ratings = df_ratings.pivot(index = 'userId', columns ='movieId', values = 'rating').fillna(0)
Ratings.head()

# Find out number of missing ratings

#df_ratings[df_ratings['movieId'].isna()]['movieId'].shape

#df_ratings.isnull().sum()

# def missing_zero_values_table(df):
#        zero_val = (df == 0.00).astype(int).sum(axis=0)
#        mis_val = df.isnull().sum()
#        mis_val_percent = 100 * df.isnull().sum() / len(df)
#        mz_table = pd.concat([zero_val, mis_val, mis_val_percent], axis=1)
#        mz_table = mz_table.rename(
#        columns = {0 : 'Zero Values', 1 : 'Missing Values', 2 : '% of Total Values'})
#        mz_table['Total Zero Missing Values'] = mz_table['Zero Values'] + mz_table['Missing Values']
#        mz_table['% Total Zero Missing Values'] = 100 * mz_table['Total Zero Missing Values'] / len(df)
#        mz_table['Data Type'] = df.dtypes
#        mz_table = mz_table[
#            mz_table.iloc[:,1] != 0].sort_values(
#        '% of Total Values', ascending=False).round(1)
#        print ("Your selected dataframe has " + str(df.shape[1]) + " columns and " + str(df.shape[0]) + " Rows.\n"      
#            "There are " + str(mz_table.shape[0]) +
#              " columns that have missing values.")
#         mz_table.to_excel('D:/sampledata/missing_and_zero_values.xlsx', freeze_panes=(1,0), index = False)
#        return mz_table

#missing_zero_values_table(df_ratings)

# No missing values in rating or No 0 values in rating, movieId, userId

# Fill NaN values in user_id and movie_id column with 0 - Not needed here
#df_ratings['userId'] = df_ratings['userId'].fillna(0)
#df_ratings['movieId'] = df_ratings['movieId'].fillna(0)

# Replace NaN values in rating column with average of all values - Not needed here
#ratings['rating'] = ratings['rating'].fillna(ratings['rating'].mean())

"""### Find new Matrix for users and movies to start SVD"""

#!pip install -q sklearn

#R = Ratings.as_matrix()
#R

#user_ratings_mean = np.mean(R, axis = 1)
#user_ratings_mean
#len(user_ratings_mean)
#user_ratings_mean.reshape(-1, 1)
#Ratings_demeaned = R - user_ratings_mean.reshape(-1, 1)

print(len(df_ratings))
print(float(n_users*n_movies))

sparsity = round(1.0 - len(df_ratings) / float(n_users * n_movies), 3)
print('The sparsity level of MovieLens1M dataset is ' +  str(sparsity * 100) + '%')

# Using scipy to perform SVD

#Scipy and Numpy both have functions to do the singular value decomposition. we use Scipy function svds 
#because it let's one choose how many latent factors to use to approximate the original ratings matrix 
#(instead of having to truncate it after).

from scipy.sparse.linalg import svds
U, sigma, Vt = svds(Ratings_demeaned, k = 50)

#U.shape # (6040,50)
#Vt.shape # 50,3706

# Since we are leveraging matrix multiplication to get predictions, next we convert the Î£ (now are values) to 
#the diagonal matrix form.

sigma = np.diag(sigma)

#sigma.shape #(50,50)

"""###Recommender system built here"""

# Making predictions 

# need to add the user means back to get the actual star ratings prediction.

all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.reshape(-1, 1)

# With the predictions matrix for every user, I can build a function to recommend movies for any user.

preds = pd.DataFrame(all_user_predicted_ratings, columns = Ratings.columns)
preds.head()

#  function to return the movies with the highest predicted rating that the specified user hasn't already rated

# didn't use any explicit movie content features (such as genre or title) but will merge in that information to get a 
# more complete picture of the recommendations.

def recommend_movies(predictions, userID, movies, original_ratings, num_recommendations):
    
    # Get and sort the user's predictions
    user_row_number = userID - 1 # User ID starts at 1, not 0
    sorted_user_predictions = preds.iloc[user_row_number].sort_values(ascending=False) # User ID starts at 1
    
    # Get the user's data and merge in the movie information.
    user_data = original_ratings[original_ratings.userId == (userID)]
    user_full = (user_data.merge(movies, how = 'left', left_on = 'movieId', right_on = 'movieId').
                     sort_values(['rating'], ascending=False)
                 )

    print('User {0} has already rated {1} movies.'.format(userID, user_full.shape[0]))
    print('Recommending highest {0} predicted ratings movies not already rated.'.format(num_recommendations))
    
    # Recommend the highest predicted rating movies that the user hasn't seen yet.
    recommendations = (movies[~movies['movieId'].isin(user_full['movieId'])].
         merge(pd.DataFrame(sorted_user_predictions).reset_index(), how = 'left',
               left_on = 'movieId',
               right_on = 'movieId').
         rename(columns = {user_row_number: 'Predictions'}).
         sort_values('Predictions', ascending = False).
                       iloc[:num_recommendations, :-1]
                      )

    return user_full, recommendations

  
  
already_rated, predictions = recommend_movies(preds, 1310, df_movies, df_ratings, 20)
  
# I will also return the list of movies the user has already rated, for the sake of comparison.

already_rated.head(20)

predictions

"""### Model Evaluation"""

#Instead of manullay building SVD based Recsys, Use the Surprise library that provided various ready-to-use powerful 
#prediction algorithms including (SVD) to evaluate 
#its RMSE (Root Mean Squared Error) on the MovieLens dataset. It is a Python scikit building and analyzing 
#recommender systems.

#!pip install -q surprise

# Import libraries from Surprise package
from surprise import Reader, Dataset, SVD, NMF, evaluate

# Load Reader library
#reader = Reader()

# Load ratings dataset with Dataset library
#data = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)

# Split the dataset for 5-fold evaluation
#data.split(n_folds=5)

# Use the SVD,NMF algorithm.
algo = NMF()

# Compute the RMSE of the algorithm.
evaluate(algo, data, measures=['RMSE'])

# SVD : .8725 .8723 .8756 .8748 .8735 Mean : .8737
# NMF : .9166 .9151 .9198 .9164 .9143 Mean : .9164

"""### Train on complete dataset to predict recommended movies"""

#trainset = data.build_full_trainset()
#algo.train(trainset)

# Check ratings currently given by randomly selected person

df_ratings[df_ratings['userId'] == 1310]
# Rated total 24 movies

# Now let's use NMF to predict the rating that User with ID 1310 will give to a random movie (let's say with Movie ID 1994) :
  
algo.predict(1310, 1961)